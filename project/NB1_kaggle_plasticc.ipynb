{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle_plasticc\n",
    "\n",
    "This Jupyter Notebook explores the Kaggle PLAsTiCC dataset and competition. The competition and dataset can be found at https://www.kaggle.com/c/PLAsTiCC-2018 \n",
    "\n",
    "This notebook can be found at  https://github.com/cwinsor/kaggle_plasticc\n",
    "\n",
    "PLAsTiCC is the Kaggle competition for LSST (Large Synoptic Survey Telescope).  The telescope, under construction at a remote mountaintop in Peru, will capture alltogether new data from the night skies, and analyze that data using alltogether new techniques.\n",
    "\n",
    "LSST performs **\"time domain astronomy\"** which means it is looking for short-term events called \"transients\". There are dozens of known classes of transients - single-occurrence events like novae and micro-lensing, periodic events like eclipsing binaries. Micro-lensing is the way astronomers discover planets orbiting stars in other galaxies - like earth in another galaxy!\n",
    "\n",
    "LSST is vast in its ability to observe. LSST will capture the entire southern hemisphere every 4 nights searching for transients. **LSST researchers expect there are about 1M transient events every night.**\n",
    "\n",
    "Finding those events in the vast quantities of data requires new technique in classification.  LSST will is exploring new boundaries in automated classification, and that is the goal of PLAsTiCC.  **The goal for LSST is to flag a one-time event (start of a Novae) within 60 minutes of observation** and notify other astronomers so they start closer observation. And that's the reason for PLAsTiCC - to establish tools for classification that can handle data on this scale.  \n",
    "\n",
    "That is Pretty Cool Stuff!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Plan\n",
    "The steps in my investigation of PLAsTiCC are planned to be:\n",
    "\n",
    "a) Review the \"starter Kit\"   https://www.kaggle.com/michaelapers/the-plasticc-astronomy-starter-kit\n",
    "b) Follow the example     https://www.kaggle.com/michaelapers/the-plasticc-astronomy-classification-demo \n",
    "\n",
    "The example starts by using the Cesium library to \"featurize\" the timeseries data\n",
    "\n",
    "So we take a dive into Cesium - library to \"featurize\" time-series data\n",
    "good example !\n",
    "http://cesium-ml.org/docs/auto_examples/plot_EEG_Example.html#sphx-glr-auto-examples-plot-eeg-example-py \n",
    "\n",
    "The example also references/uses a library to establish period of a repeating signal that is sampled infrequently\n",
    "<interesting stuff, and very useful>\n",
    "<need to revisit this>\n",
    "    \n",
    "The example then reduces the number of features and evaluates using correlation, PCA, Confusion Matrix\n",
    "<more work needed here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Dataset:\n",
    "* The dataset has two parts\n",
    "* \"metadata\" - aka - red shift and color frequency bands for each item\n",
    "* \"luminescence\" - aka - the brightness for each item\n",
    "\n",
    "To simplify things:\n",
    "* Start with just <luminescence> - no metadata\n",
    "* Start with just \"galactic\" data (our galaxy)\n",
    "* Above will simplify things a lot\n",
    "    \n",
    "My plan is:\n",
    "1) Wrangle and get familiar with the data - a bit\n",
    "* For each of the (N) categorical data types - investigate ONE instance.\n",
    "* This is a single star in our our galaxy which we know is an \"XYZ\".\n",
    "* Investigate the properties and/or behavior of that star.\n",
    "* Confirm we know a little about each category and what their observable properties are.\n",
    "\n",
    "2) Investigate approaches taken by other Kagglers:\n",
    "* This may take some time - review \"top 100\" models - categorize as to what they were predicting, the approach taken.\n",
    "* Create a table.\n",
    "\n",
    "3) Investigate approaches taken by other Astronomers ....\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the Discussion Board\n",
    "https://www.kaggle.com/c/PLAsTiCC-2018/discussion\n",
    "\n",
    "\n",
    "* Paper on the Performance Metric **good read** https://arxiv.org/abs/1809.11145\n",
    " * Introduction:\n",
    "   * The \"Performance Metric\" is chosen to prefer Probabilistic Classification vs Determinimistic Classification\n",
    "     * Probabillistic Classification delivers a probability distribution over class values.  Deterministic Classification delivers a MAP (max a-priori) for a class value.\n",
    "     * The reasons are:\n",
    "       * Resources for analysis are limited - model users need model that assists in making tradeoff decisions\n",
    "       * Data can be incomplete - a probabilistic model allows inference on incomplete data\n",
    "       * Recent research on hierarchical inference - this is a hierarchical model\n",
    "     * ***I believe they are missing the intent which is to separate representation (the model) from inference (the querying). Reference Koller.***\n",
    "   * In the summary they conclude\n",
    "     * \"we sought a metric (...) that avoids deterministic labels (preferring) one with strong performance across all classes\"\n",
    "     * Brier score vs log-loss.   Sum-of-sqare differences between probabilities vs Log-loss\n",
    "     * They also state \"Both metrics are susceptible to rewarding a classifier that performs well on the most prevalent class and poorly on all others\" \n",
    "     * In summary \"we select a per-class weighted log-loss as the optimal choice for PLAsTiCC\"\n",
    "   * Note that log-loss is Entropy where p() is the conditional probability p(m|d) where m = light_curve has true class m and d is the data\n",
    "   * They also use cross-entropy which they describe as increase in disorder\n",
    "   * **several references here** including the KullbackLeibler Divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Wikipedia:\n",
    "![Cross_Entropy,bits_to_transmit_dist_1_using_dist_2](images/pic01_cross_entropy_def.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"foo\",\"bar\"](images/pic02_cross_entropy_def2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the article:\n",
    "![\"foo\",\"bar\"](images/pic03_metric_definitions_and_indicator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes..\n",
    "note they have not yet defined the metric Qn ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining cross-entropy as the metric\n",
    "![\"foo\",\"bar](images/pic04_metric_value_definition_using_log_loss_cross_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
