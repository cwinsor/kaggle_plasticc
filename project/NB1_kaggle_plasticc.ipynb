{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle_plasticc\n",
    "\n",
    "This Jupyter Notebook explores the Kaggle PLAsTiCC dataset and competition. The competition and dataset can be found at https://www.kaggle.com/c/PLAsTiCC-2018 \n",
    "\n",
    "This notebook can be found at  https://github.com/cwinsor/kaggle_plasticc\n",
    "\n",
    "PLAsTiCC is the Kaggle competition for LSST (Large Synoptic Survey Telescope).  The telescope, under construction at a remote mountaintop in Peru, will capture alltogether new data from the night skies, and analyze that data using alltogether new techniques.\n",
    "\n",
    "LSST performs **\"time domain astronomy\"** which means it is looking for short-term events called \"transients\". There are dozens of known classes of transients - single-occurrence events like novae and micro-lensing, periodic events like eclipsing binaries. Micro-lensing is the way astronomers discover planets orbiting stars in other galaxies - like earth in another galaxy!\n",
    "\n",
    "LSST is vast in its ability to observe. LSST will capture the entire southern hemisphere every 4 nights searching for transients. **LSST researchers expect there are about 1M transient events every night.**\n",
    "\n",
    "Finding those events in the vast quantities of data requires new technique in classification.  LSST will is exploring new boundaries in automated classification, and that is the goal of PLAsTiCC.  **The goal for LSST is to flag a one-time event (start of a Novae) within 60 minutes of observation** and notify other astronomers so they start closer observation. And that's the reason for PLAsTiCC - to establish tools for classification that can handle data on this scale.  \n",
    "\n",
    "That is Pretty Cool Stuff!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Plan\n",
    "The steps in my investigation of PLAsTiCC are:\n",
    "\n",
    "1. Review the \"starter Kit\" https://www.kaggle.com/michaelapers/the-plasticc-astronomy-starter-kit\n",
    "2. Review example from the host https://www.kaggle.com/michaelapers/the-plasticc-astronomy-classification-demo *(note I moved my notes from this to the bottom - it is not consistent with the leaderboard techniques)*\n",
    "3. ***Review leaderboard kernels (from Discusison Board)***\n",
    "4. Review the \"Info For the Challenge\" from hosts - describing the Performance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The example from host\n",
    "The example starts by using the Cesium library to \"featurize\" the timeseries data\n",
    "\n",
    "So we take a quick dive into Cesium - a library to \"featurize\" time-series data\n",
    "good example !\n",
    "http://cesium-ml.org/docs/auto_examples/plot_EEG_Example.html#sphx-glr-auto-examples-plot-eeg-example-py \n",
    "\n",
    "The example also references/uses a library to establish period of a repeating signal that is sampled infrequently\n",
    "<interesting stuff, and very useful>\n",
    "<need to revisit this>\n",
    "    \n",
    "The example then reduces the number of features and evaluates using correlation, PCA, Confusion Matrix\n",
    "<more work needed here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Leaderboard Models (posted on Discussion Board)...\n",
    "https://www.kaggle.com/c/PLAsTiCC-2018/discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"20th Place Solution\" (Giba)\n",
    "https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75262#latest-527064\n",
    "* Summary:\n",
    " * Stack ensemble and a blend of 4x LGB and 2x SVC models trained using different subsets of features.\n",
    "* Features:\n",
    "  * tried > 1000, pruned down to 250\n",
    "  * aggregations and statistics, feets lib, coefficients of linear...\n",
    "* Models:\n",
    "  * for each - augmented train set w/ noise (?)\n",
    "  * Model 1 - multi-class LGB\n",
    "  * Model 2 - multi-class LGB\n",
    "  * Model 3 - multi-class LGB\n",
    "  * Model 4 - multi-*label* LGB\n",
    "  * Model 5 - multi-*label* SVC\n",
    "  * Model 6 - multi-class LGB\n",
    "  * Model 7 - multi-*label* LGB\n",
    "  * Stack Ensemble 1\n",
    "  * Stack Ensemble 2\n",
    "  * Stack Ensemble 3\n",
    "  * Blend Stack Ensemble 1,2,3\n",
    "  * Class 99 - used \"Scirpus\" equation \n",
    "* What Didn't work:\n",
    "  * ...\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"9th place solution\" (Albert Garreta)\n",
    "* https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75316#latest-495584\n",
    "* In Summary:\n",
    "  * careful feature engineering and ensembling\n",
    "  * stacking predictions of lgb, catboost, and nn, then a weighted average with previous best submission\n",
    "* First-level Models\n",
    "  * a single lgb (teammates pursued other)\n",
    "* Features\n",
    "  * 8000 features\n",
    "  * prune to (80 + 130) using LGB\n",
    "    * @manugangler's kernel fitting light curves to microlensing events\n",
    "    * distmod/log10(hostgal_photoz)\n",
    "    * log_10( luminosity_mean )\n",
    "    * used different sets of time observations (all, detected, undetected)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"2nd-Place Solution Notes\" (Silogram)\n",
    "* https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75059#latest-462457\n",
    "* In Summary\n",
    "  * Neual net (details in discussion)\n",
    "  * meta-model as GBDT model(depth = 2)\n",
    "* Notes:\n",
    "  * The Gap: they did not have \"LV vs CV\" (?) problem\n",
    "  * Class 99 - left to the end\n",
    "  * Final ensemble shallow - depth=2\n",
    "* Feature Engineering:\n",
    "   * \"Detected\" flag\n",
    "   * \"hostgalspez\"\n",
    "   * flux adjustment\n",
    "   * MJD adjustment\n",
    "* Augmentation - tried a variety - helped NN but not LGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is \"LGB\" ?\n",
    "* \"Linear Gradient Boosting\"\n",
    "* https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "* In summary\n",
    "  * LGB starts with a weak model.\n",
    "  * It then considers that a better model would happen by linearly adding a \"correction\".\n",
    "  * Simple algebra - the \"correction\" is the difference between the true class values, and the predicted class values\n",
    "  * So - train a new model to predict the \"correction\"\n",
    "  * that's the general idea - see Wikipedia!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"14th place solution\" (Belinda Trotta)\n",
    "* https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75054#latest-448552\n",
    "* ***well documented - a good starting point?***\n",
    "* ***pdf with details is available***\n",
    "* ***source is available on github***\n",
    "  \n",
    "* Summary:\n",
    "\n",
    "  * LightGBM\n",
    "  * run in 6 hours on a 24GB laptop\n",
    "  * only elementary operations (no curve fitting or optimization - keeps runtime down)\n",
    "* Feature/engineering\n",
    "  * remove noise from flux (Bayes approach)\n",
    "  * create scaled version of flux \n",
    "  * peaks - added features to capture the behaviour around the peak\n",
    "  * \"Understanding how to optimise the metric\" ? \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \"Solution #5 tidbits (revised with code)\" (CPMP)\n",
    "* https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75050#latest-447982\n",
    "* ***good feature engineering***\n",
    "* ***pdf and code on github***\n",
    "* Summary:\n",
    "  * lightgbm (almost exclusively)\n",
    "  \n",
    "* Feature Engineering Approach\n",
    "  * hand crafted from light curves (used Pandas)\n",
    "  * did NOT use packages (light gatspy, cesium, tsfresh) because they were \"way slower\" and \"not as good\"\n",
    "* Features\n",
    "  * Std, skewness, kurtosis. **Goal = differentiate curves w/peaks (supernova) from others. Also captures shape of peak**\n",
    "  * Ratios e.g. max passband divide by largest. **Gives a view of the spectra.**\n",
    "  * Features based on difference between successive measurements. **Isolates rapidly varying sources from others**\n",
    "  * Magnitude of Flux (need to compute this to start with)\n",
    "  * MJD diff (days) feature - shared by Sionkowski in forum\n",
    "  * Normalized flux\n",
    "  * Curve fitting (?)\n",
    "  \n",
    "* Objective Function\n",
    "  * ***this is important and I don't understand it*** he is looking at the competition performance metric\n",
    "* Training-time Augmentstion\n",
    "* Stacking (\"an immediate benefit of teaming\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is \"LightGBM\" ?\n",
    "https://lightgbm.readthedocs.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #13 Solution, true story: tries and fails (Blonde)\n",
    "* https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75134#latest-445370\n",
    "* ***first-hand sense of fun - first kaggle?!***\n",
    "* Summmary:\n",
    "  * LGBM\n",
    "  * feature design and selection\n",
    "  * augmentation\n",
    "  * \"Not much ensembling, 50/50 blend of two LGBM models\"\n",
    "* Features:\n",
    "  * Magnitude (aggregated and per-band)\n",
    "  * Parametric curve fittings (Bazin - reference)\n",
    "  * extended Baizen\n",
    "  * cesium (basic ratios, std, skew)\n",
    "  * checked all features from \"feets\" (e.g. CAR tau from feets).  Optimized this 10x https://feets.readthedocs.io/en/latest/tutorial.html\n",
    "\n",
    "* \"Battling over-fitting\"\n",
    "  * augmented training (flux)\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th Place Solution with Github Repo (Ahmet Erdem)\n",
    "https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75011#latest-444878\n",
    "* ***code on github***\n",
    "* Summary:\n",
    "  * a blend of LGB, NN and several stacking models\n",
    "* Features that worked:\n",
    "  * Ratios (passband / all passbands)\n",
    "  * Stacking e.g. Simple Logistic Regression on LGB  <-- maybe a good exercise?\n",
    "  * sub-model (trained model to predict Hostgalspecz, then used model prediction as feature)\n",
    "  * normal and log-transformed (both) on Neual Net (allows +,-,*,/)\n",
    "  * Baizen (light-curve fitting)\n",
    "  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12th Place Solution (Daniel Bi)\n",
    "* Summary:\n",
    "  * ensemble of LightGBM, XGBoost, and binary classifiers.\n",
    "* Feature Engineering\n",
    "  * light curves - 50-60 features. Did both passband-level and object-level features.  Did NOT use libraries\n",
    "  * For frequency extraction - used kernel provided by third-party, groupd into 20 bins\n",
    "  * Baizen (light curve fitting)\n",
    "  * flux adjustments (a variety) and \n",
    "* Models:\n",
    "  * three models: galactic, extragalactic with specz, and extragalactic without specz\n",
    "  \"Passband Level Model\" = LightGBM on light curve only\n",
    "  \"Object Level Model\" = LightGBM  100 features (galactic), 140 features (extragalactic)\n",
    "* Ensemble\n",
    "  * LightGBM, XGBoost and binary classification predictions\n",
    "  \n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of 1st place solution (Kyle Boone)\n",
    "https://www.kaggle.com/c/PLAsTiCC-2018/discussion/75033#latest-457546\n",
    "* an astrophysicist\n",
    "* code available on https://github.com/kboone/avocado\n",
    "* Summary   \n",
    "  * most of work was put into separating super-novae apart because (\"everything else was fairly easy to tell apart\")\n",
    "  * ***-> in other words - use time wisely - ML will solve part of the prediction easily - focus where ML needs help***\n",
    "  * augmented training set by \"degrading the well-observed lightcurves in the training set to match the properties of the test set\".\n",
    "  * ***-> in other words - the training set only had a few examples of well obseved light curves so it is necessary to augment these to prevent over-fit***\n",
    "  * use gaussian process to predict light curves\n",
    "  * ***-> what does this mean? - review code***\n",
    "  * measured 200 features on raw data and Gaussian process predictions\n",
    "  * trained single LGBM model with 5-fold cross-validation\n",
    "* ***-> There is a WEBSITE on \"Avocado\" https://avocado-classifier.readthedocs.io/en/latest/***\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------ next steps - review code from examples! -----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------- end ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What follows are raw notes from my review of the paper on Performance Metrics.  Very interesting but too much detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Info for the Challenge\"\n",
    "https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67376#latest-439538\n",
    "* Paper on the Performance Metric **good read** https://arxiv.org/abs/1809.11145\n",
    " * Introduction:\n",
    "   * The \"Performance Metric\" is chosen to prefer Probabilistic Classification vs Determinimistic Classification\n",
    "     * Probabillistic Classification delivers a probability distribution over class values.  Deterministic Classification delivers a MAP (max a-priori) for a class value.\n",
    "     * The reasons are:\n",
    "       * Resources for analysis are limited - model users need model that assists in making tradeoff decisions\n",
    "       * Data can be incomplete - a probabilistic model allows inference on incomplete data\n",
    "       * Recent research on hierarchical inference - this is a hierarchical model\n",
    "     * ***I believe they are missing the intent which is to separate representation (the model) from inference (the querying). Reference Koller.***\n",
    "   * In the summary they conclude\n",
    "     * \"we sought a metric (...) that avoids deterministic labels (preferring) one with strong performance across all classes\"\n",
    "     * Brier score vs log-loss.   Sum-of-sqare differences between probabilities vs Log-loss\n",
    "     * They also state \"Both metrics are susceptible to rewarding a classifier that performs well on the most prevalent class and poorly on all others\" \n",
    "     * In summary \"we select a per-class weighted log-loss as the optimal choice for PLAsTiCC\"\n",
    "   * Note that log-loss is Entropy where p() is the conditional probability p(m|d) where m = light_curve has true class m and d is the data\n",
    "   * They also use cross-entropy which they describe as increase in disorder\n",
    "   * **several references here** including the KullbackLeibler Divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Wikipedia:\n",
    "![Cross_Entropy,bits_to_transmit_dist_1_using_dist_2](images/pic01_cross_entropy_def.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"foo\",\"bar\"](images/pic02_cross_entropy_def2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the article:\n",
    "![\"foo\",\"bar\"](images/pic03_metric_definitions_and_indicator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes..\n",
    "note they have not yet defined the metric Qn ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining cross-entropy as the metric\n",
    "![\"foo\",\"bar](images/pic04_metric_value_definition_using_log_loss_cross_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
